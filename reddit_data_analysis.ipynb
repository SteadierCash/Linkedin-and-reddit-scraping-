{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the database\n",
    "cnx = mysql.connector.connect(\n",
    "    host=\"XXX\",\n",
    "    user=\"XXX\",\n",
    "    password=\"XXX\",\n",
    "    database=\"XXX\"\n",
    ")\n",
    "\n",
    "# Create a cursor to execute SQL statements\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Execute a SELECT statement to fetch data from the table\n",
    "query = \"SELECT * FROM reddit_posts\"\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch all the rows from the result set\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Get the column names from the cursor description\n",
    "columns = [column[0] for column in cursor.description]\n",
    "\n",
    "# Convert the rows to a dataframe with the specified column names\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "# Close the cursor and the database connection\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post_titles = np.array(df['post_title'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing post tiles and counting words\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "all_words = []\n",
    "\n",
    "for title in post_titles:\n",
    "    words = word_tokenize(title)\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n",
    "    words = [word for word in words if word]\n",
    "    all_words += words\n",
    "\n",
    "frequency_dist = FreqDist(all_words)\n",
    "\n",
    "for word, frequency in frequency_dist.most_common():\n",
    "    print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_comments = np.concatenate((df['top_comment_one'], df['top_comment_two'], df['top_comment_three']))\n",
    "post_comments = [comment for comment in post_comments if comment != '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing post comments\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "all_words_comments = []\n",
    "\n",
    "for comment in post_comments:\n",
    "    comm = word_tokenize(comment)\n",
    "    comm = [c.lower() for c in comm if c.lower() not in stop_words]\n",
    "    comm = [re.sub(r'[^\\w\\s]', '', c) for c in comm]\n",
    "    comm = [c for c in comm if c]\n",
    "    all_words_comments += comm\n",
    "\n",
    "frequency_dist = FreqDist(all_words_comments)\n",
    "\n",
    "for c, frequency in frequency_dist.most_common():\n",
    "    print(f\"{c}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's build a simple lda model to search for topics and compare them to linked descriptions in the future\n",
    "post_bodies = df['post_body']\n",
    "\n",
    "#gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "  \n",
    "#spacy\n",
    "import spacy\n",
    "\n",
    "#vis\n",
    "import pyLDAvis \n",
    "import pyLDAvis.gensim\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_tags = ['NOUN','ADJ','VERB','ADV']):\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        if not isinstance(text, str):\n",
    "            print(f\"Found non-string element: {text}\")\n",
    "        else:\n",
    "            tokens = nlp(text)\n",
    "            lemmas = [token.lemma_ for token in tokens if token.pos_ in allowed_tags]\n",
    "            texts_out.append(\" \".join(lemmas))\n",
    "    return texts_out\n",
    "\n",
    "lemmatized_texts = lemmatization(post_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text)\n",
    "        final.append(new)\n",
    "    return final\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for text in data_words:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model \n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                            id2word = id2word,\n",
    "                                            num_topics = 10,\n",
    "                                            random_state = 50,\n",
    "                                            update_every = 1,\n",
    "                                            chunksize = 50,\n",
    "                                            passes = 10,\n",
    "                                            alpha = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f'Topic: {idx} \\nWords: {topic}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now i am gonna create bigrams/ trigrams as well as remove frequent words that do not bring much to my analysis\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# create bigram phrases\n",
    "bigram_phrases = Phrases(data_words, min_count=5, threshold=80)\n",
    "\n",
    "# transform the corpus using the bigram phrases\n",
    "data_bigrams = [bigram_phrases[doc] for doc in data_words]\n",
    "\n",
    "# create trigram phrases\n",
    "trigram_phrases = Phrases(data_bigrams, min_count=5, threshold=80)\n",
    "\n",
    "# transform the corpus using the trigram phrases\n",
    "data_trigrams = [trigram_phrases[bigram_doc] for bigram_doc in data_bigrams]\n",
    "\n",
    "# create a Phraser object for the trigram phrases (optional, but recommended for faster processing)\n",
    "trigram_phraser = Phraser(trigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's remove the frequent unnecessary rows\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Create a dictionary of the trigrams in the data\n",
    "id2word = corpora.Dictionary(data_trigrams)\n",
    "\n",
    "# Convert the data to a bag-of-words corpus\n",
    "texts = data_trigrams \n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# Compute the TF-IDF scores for the corpus\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "# Define a threshold for low-value words\n",
    "low_value = 0.03\n",
    "\n",
    "# Initialize lists for storing low-value words and missing words\n",
    "words = []\n",
    "words_missing_in_tfidf = []\n",
    "\n",
    "# Loop over each document in the corpus\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    \n",
    "    # Initialize a list for storing low-value words\n",
    "    low_value_words = []\n",
    "    \n",
    "    # Get the IDs of words with nonzero TF-IDF scores\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    \n",
    "    # Get the IDs of all words in the document\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    \n",
    "    # Find all words with a TF-IDF score below the threshold\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    \n",
    "    # Add low-value words and missing words to the list of words to be dropped\n",
    "    drops = low_value_words + words_missing_in_tfidf\n",
    "    \n",
    "    # Add dropped words to the list of words\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    \n",
    "    # Find all words that are missing from the TF-IDF model\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
    "    \n",
    "    # Remove all low-value and missing words from the document\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]  \n",
    "    \n",
    "    # Replace the original document with the cleaned-up version\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new model\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                            id2word = id2word,\n",
    "                                            num_topics = 10,\n",
    "                                            random_state = 50,\n",
    "                                            update_every = 1,\n",
    "                                            chunksize = 40,\n",
    "                                            passes = 10,\n",
    "                                            alpha = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization \n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds = 'mmds', R = 10)\n",
    "vis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_prj-SRynVlFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34304ceff4189be5ec3888addaf2a860e88df2f436b7958182f0d19632a0f568"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
